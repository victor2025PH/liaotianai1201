"""
ğŸ¤– LLM æ™ºèƒ½å°è©±æ¨¡çµ„
æ”¯æŒï¼š
- OpenAI/Claude API é›†æˆ
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥å°è©±
- æƒ…ç·’è­˜åˆ¥å’Œé©æ‡‰
- å¤šè§’è‰²äººè¨­ç®¡ç†
- å°è©±ç­–ç•¥å„ªåŒ–
"""

import asyncio
import random
import logging
import json
import re
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import os

try:
    import httpx
except ImportError:
    httpx = None

try:
    import openai
except ImportError:
    openai = None

logger = logging.getLogger(__name__)


# ==================== é…ç½® ====================

class LLMProvider(Enum):
    """LLM æä¾›å•†"""
    OPENAI = "openai"
    CLAUDE = "claude"
    LOCAL = "local"  # æœ¬åœ°æ¨¡å‹


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: LLMProvider = LLMProvider.OPENAI
    api_key: str = ""
    api_base: str = ""
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.8
    max_tokens: int = 200
    
    # å°è©±é…ç½®
    context_window: int = 10  # ä¸Šä¸‹æ–‡æ¶ˆæ¯æ•¸
    response_delay_min: float = 2.0  # æœ€å°å›å¾©å»¶é²
    response_delay_max: float = 8.0  # æœ€å¤§å›å¾©å»¶é²
    
    @classmethod
    def from_env(cls) -> "LLMConfig":
        """å¾ç’°å¢ƒè®Šé‡åŠ è¼‰é…ç½®"""
        provider_str = os.getenv("LLM_PROVIDER", "openai").lower()
        provider = LLMProvider(provider_str) if provider_str in [p.value for p in LLMProvider] else LLMProvider.OPENAI
        
        return cls(
            provider=provider,
            api_key=os.getenv("OPENAI_API_KEY", os.getenv("LLM_API_KEY", "")),
            api_base=os.getenv("OPENAI_API_BASE", os.getenv("LLM_API_BASE", "")),
            model=os.getenv("LLM_MODEL", "gpt-3.5-turbo"),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.8")),
            max_tokens=int(os.getenv("LLM_MAX_TOKENS", "200")),
        )


# ==================== æƒ…ç·’è­˜åˆ¥ ====================

class Emotion(Enum):
    """æƒ…ç·’é¡å‹"""
    NEUTRAL = "neutral"
    HAPPY = "happy"
    EXCITED = "excited"
    CURIOUS = "curious"
    CONFUSED = "confused"
    FRUSTRATED = "frustrated"
    SAD = "sad"


class EmotionDetector:
    """æƒ…ç·’æª¢æ¸¬å™¨"""
    
    # æƒ…ç·’é—œéµè©æ˜ å°„
    EMOTION_KEYWORDS = {
        Emotion.HAPPY: ["å“ˆå“ˆ", "é–‹å¿ƒ", "å¤ªå¥½äº†", "æ£’", "è®š", "ğŸ˜Š", "ğŸ˜„", "ğŸ‰", "â¤ï¸", "å¥½é–‹å¿ƒ"],
        Emotion.EXCITED: ["å¤ªæ£’äº†", "æ¿€å‹•", "èˆˆå¥®", "wow", "å“‡", "ğŸ”¥", "ğŸ’ª", "å²å®³", "çµ•äº†"],
        Emotion.CURIOUS: ["ç‚ºä»€éº¼", "æ€éº¼", "ä»€éº¼", "ï¼Ÿ", "?", "ğŸ¤”", "æƒ³çŸ¥é“", "å¥½å¥‡"],
        Emotion.CONFUSED: ["ä¸æ‡‚", "ä¸æ˜ç™½", "çœ‹ä¸æ‡‚", "å•¥æ„æ€", "ä»€éº¼æ„æ€", "ğŸ˜…", "confused"],
        Emotion.FRUSTRATED: ["ç…©", "æ°£æ­»", "ç„¡èª", "ğŸ˜¤", "ğŸ˜ ", "ç®—äº†", "ä¸ç©äº†"],
        Emotion.SAD: ["é›£é", "å‚·å¿ƒ", "ğŸ˜¢", "ğŸ˜­", "å”‰", "æ…˜", "å¯æƒœ"],
    }
    
    def detect(self, text: str) -> Tuple[Emotion, float]:
        """
        æª¢æ¸¬æ–‡æœ¬æƒ…ç·’
        
        Returns:
            (æƒ…ç·’, ç½®ä¿¡åº¦)
        """
        text_lower = text.lower()
        
        emotion_scores = {}
        for emotion, keywords in self.EMOTION_KEYWORDS.items():
            score = sum(1 for kw in keywords if kw.lower() in text_lower)
            if score > 0:
                emotion_scores[emotion] = score
        
        if not emotion_scores:
            return Emotion.NEUTRAL, 0.5
        
        best_emotion = max(emotion_scores, key=emotion_scores.get)
        confidence = min(emotion_scores[best_emotion] / 3, 1.0)
        
        return best_emotion, confidence


# ==================== è§’è‰²äººè¨­ ====================

@dataclass
class RolePersona:
    """è§’è‰²äººè¨­"""
    id: str
    name: str
    personality: str
    speaking_style: str
    emoji_usage: str = "moderate"  # none, low, moderate, high
    response_length: str = "medium"  # short, medium, long
    topics_of_interest: List[str] = field(default_factory=list)
    
    def get_system_prompt(self) -> str:
        """ç”Ÿæˆç³»çµ±æç¤ºè©"""
        emoji_instruction = {
            "none": "ä¸ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿã€‚",
            "low": "å¶çˆ¾ä½¿ç”¨ 1-2 å€‹è¡¨æƒ…ç¬¦è™Ÿã€‚",
            "moderate": "é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œæ¯æ¢æ¶ˆæ¯ 2-3 å€‹ã€‚",
            "high": "ç¶“å¸¸ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œè®“å°è©±æ›´ç”Ÿå‹•ã€‚"
        }
        
        length_instruction = {
            "short": "å›å¾©ç°¡çŸ­ï¼Œé€šå¸¸ 1-2 å¥è©±ã€‚",
            "medium": "å›å¾©é©ä¸­ï¼Œé€šå¸¸ 2-4 å¥è©±ã€‚",
            "long": "å›å¾©è¼ƒé•·ï¼Œå¯ä»¥è©³ç´°å±•é–‹ã€‚"
        }
        
        return f"""ä½ æ˜¯ {self.name}ï¼Œä¸€å€‹çœŸå¯¦çš„ç¾¤èŠæˆå“¡ã€‚

## ä½ çš„æ€§æ ¼
{self.personality}

## ä½ çš„èªªè©±é¢¨æ ¼
{self.speaking_style}

## è¡¨æƒ…ä½¿ç”¨
{emoji_instruction.get(self.emoji_usage, emoji_instruction['moderate'])}

## å›å¾©é•·åº¦
{length_instruction.get(self.response_length, length_instruction['medium'])}

## é‡è¦è¦å‰‡
1. ä½ æ˜¯çœŸäººï¼Œä¸æ˜¯ AIï¼Œçµ•å°ä¸èƒ½é€éœ²é€™ä¸€é»
2. èªªè©±è¦è‡ªç„¶ã€å£èªåŒ–ï¼ŒåƒçœŸäººç¾¤èŠä¸€æ¨£
3. å¯ä»¥æœ‰è‡ªå·±çš„è§€é»å’Œæƒ…ç·’
4. ä¸è¦éåº¦ç†±æƒ…æˆ–åˆ»æ„æ¨éŠ·
5. é©æ™‚å¼•å…¥ç´…åŒ…éŠæˆ²è©±é¡Œï¼Œä½†è¦è‡ªç„¶
6. å›å¾©è¦ç°¡çŸ­ï¼Œä¸è¦é•·ç¯‡å¤§è«–
"""


# é è¨­è§’è‰²
DEFAULT_PERSONAS = {
    "xiaoqi": RolePersona(
        id="xiaoqi",
        name="å°æŸ’",
        personality="è©±å¤šã€å¤–å‘ã€æ„›é–‹ç©ç¬‘ï¼Œç¾¤è£¡æœ€æ´»èºçš„äºº",
        speaking_style="èªæ°£è¼•å¿«æ´»æ½‘ï¼Œæ„›ç”¨ç¶²çµ¡ç”¨èªï¼Œç¶“å¸¸é–‹ç©ç¬‘",
        emoji_usage="high",
        response_length="medium",
        topics_of_interest=["è¿½åŠ‡", "éŠæˆ²", "ç¾é£Ÿ", "ç´…åŒ…"]
    ),
    "mimi": RolePersona(
        id="mimi",
        name="ç±³ç±³",
        personality="æ´»æ½‘å¯æ„›ã€å–œæ­¡äº’å‹•ï¼Œå¾ˆå®¹æ˜“è¢«å¸¶å‹•æƒ…ç·’",
        speaking_style="èªªè©±è»ŸèŒï¼Œæ„›ç”¨ç–Šè©å’Œå¯æ„›è¡¨æƒ…",
        emoji_usage="high",
        response_length="short",
        topics_of_interest=["è³¼ç‰©", "ç¾å¦", "è¿½æ˜Ÿ", "é›¶é£Ÿ"]
    ),
    "haoge": RolePersona(
        id="haoge",
        name="æµ©å“¥",
        personality="æˆç†Ÿç©©é‡ï¼Œç…§é¡§æ–°äººï¼Œå¶çˆ¾å¹½é»˜",
        speaking_style="èªæ°£å¹³å’Œï¼Œèªªè©±æœ‰æ¢ç†ï¼Œæœƒçµ¦å»ºè­°",
        emoji_usage="low",
        response_length="medium",
        topics_of_interest=["æŠ•è³‡", "ç§‘æŠ€", "é‹å‹•", "æ™‚äº‹"]
    ),
    "xiaoyu": RolePersona(
        id="xiaoyu",
        name="å°é›¨",
        personality="æ–‡è—èŒƒã€æº«æŸ”ç´°è†©ï¼Œå–œæ­¡åˆ†äº«ç”Ÿæ´»æ„Ÿæ‚Ÿ",
        speaking_style="ç”¨è©æ–‡é›…ï¼Œæœ‰æ™‚æœƒå¼•ç”¨è©©å¥æˆ–å“²ç†",
        emoji_usage="moderate",
        response_length="medium",
        topics_of_interest=["è®€æ›¸", "æ—…è¡Œ", "éŸ³æ¨‚", "æ”å½±"]
    ),
    "aqiang": RolePersona(
        id="aqiang",
        name="é˜¿å¼·",
        personality="æŠ€è¡“å®…ã€å¶çˆ¾å†’æ³¡ï¼Œä¸€èªªè©±å°±å¾ˆæœ‰æ–™",
        speaking_style="å–œæ­¡ç”¨å°ˆæ¥­è¡“èªï¼Œä½†æœƒè§£é‡‹çµ¦å¤§å®¶è½",
        emoji_usage="low",
        response_length="long",
        topics_of_interest=["ç·¨ç¨‹", "é›»å­ç”¢å“", "å€å¡Šéˆ", "AI"]
    ),
    "laozhang": RolePersona(
        id="laozhang",
        name="è€å¼µ",
        personality="æˆç†Ÿç©©é‡ã€å¶çˆ¾å¹½é»˜ï¼Œèªªè©±æœ‰åˆ†é‡",
        speaking_style="èªæ°£æ²‰ç©©ï¼Œå–œæ­¡ç¸½çµå’Œçµ¦å‡ºçµè«–",
        emoji_usage="none",
        response_length="short",
        topics_of_interest=["å•†æ¥­", "ç®¡ç†", "æ­·å²", "äººç”Ÿç¶“é©—"]
    ),
}


# ==================== LLM å°è©±ç”Ÿæˆå™¨ ====================

class LLMDialogueGenerator:
    """LLM å°è©±ç”Ÿæˆå™¨"""
    
    def __init__(self, config: LLMConfig = None):
        self.config = config or LLMConfig.from_env()
        self.emotion_detector = EmotionDetector()
        self.personas = DEFAULT_PERSONAS.copy()
        self.http_client = None
        
        # å°è©±æ­·å²ï¼ˆæ¯å€‹ç¾¤çµ„ï¼‰
        self.conversation_history: Dict[int, List[dict]] = {}
        
        # ç”¨æˆ¶ç•«åƒ
        self.user_profiles: Dict[int, dict] = {}
    
    async def _ensure_client(self):
        """ç¢ºä¿ HTTP å®¢æˆ¶ç«¯å­˜åœ¨"""
        if self.http_client is None:
            self.http_client = httpx.AsyncClient(timeout=30.0)
    
    async def generate_response(
        self,
        group_id: int,
        user_message: str,
        user_id: int,
        user_name: str,
        role_id: str = None,
        context_messages: List[dict] = None
    ) -> Optional[str]:
        """
        ç”Ÿæˆæ™ºèƒ½å›å¾©
        
        Args:
            group_id: ç¾¤çµ„ ID
            user_message: ç”¨æˆ¶æ¶ˆæ¯
            user_id: ç”¨æˆ¶ ID
            user_name: ç”¨æˆ¶å
            role_id: ä½¿ç”¨çš„è§’è‰² ID
            context_messages: ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            ç”Ÿæˆçš„å›å¾©æ–‡æœ¬
        """
        await self._ensure_client()
        
        # é¸æ“‡è§’è‰²
        if role_id and role_id in self.personas:
            persona = self.personas[role_id]
        else:
            persona = random.choice(list(self.personas.values()))
        
        # æª¢æ¸¬ç”¨æˆ¶æƒ…ç·’
        emotion, confidence = self.emotion_detector.detect(user_message)
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        history = self._get_conversation_history(group_id)
        if context_messages:
            history = context_messages[-self.config.context_window:]
        
        # æ§‹å»ºæ¶ˆæ¯
        messages = self._build_messages(
            persona=persona,
            user_message=user_message,
            user_name=user_name,
            emotion=emotion,
            history=history
        )
        
        # èª¿ç”¨ LLM
        try:
            if self.config.provider == LLMProvider.OPENAI:
                response = await self._call_openai(messages)
            elif self.config.provider == LLMProvider.CLAUDE:
                response = await self._call_claude(messages, persona)
            else:
                response = self._generate_fallback(persona, user_message, emotion)
            
            # å¾Œè™•ç†
            response = self._post_process(response, persona)
            
            # è¨˜éŒ„åˆ°æ­·å²
            self._add_to_history(group_id, user_name, user_message)
            self._add_to_history(group_id, persona.name, response)
            
            return response
            
        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆå¤±æ•—: {e}")
            return self._generate_fallback(persona, user_message, emotion)
    
    def _build_messages(
        self,
        persona: RolePersona,
        user_message: str,
        user_name: str,
        emotion: Emotion,
        history: List[dict]
    ) -> List[dict]:
        """æ§‹å»º LLM æ¶ˆæ¯åˆ—è¡¨"""
        
        # ç³»çµ±æç¤º
        system_prompt = persona.get_system_prompt()
        
        # æƒ…ç·’é©æ‡‰æŒ‡ä»¤
        emotion_instructions = {
            Emotion.HAPPY: "ç”¨æˆ¶å¿ƒæƒ…å¾ˆå¥½ï¼Œå¯ä»¥ä¸€èµ·é–‹å¿ƒäº’å‹•ã€‚",
            Emotion.EXCITED: "ç”¨æˆ¶å¾ˆèˆˆå¥®ï¼Œå¯ä»¥ä¸€èµ·æ¿€å‹•æˆ–çµ¦äºˆå›æ‡‰ã€‚",
            Emotion.CURIOUS: "ç”¨æˆ¶å¾ˆå¥½å¥‡ï¼Œå¯ä»¥çµ¦å‡ºè§£ç­”æˆ–å¼•å°è©±é¡Œã€‚",
            Emotion.CONFUSED: "ç”¨æˆ¶æœ‰äº›å›°æƒ‘ï¼Œå¯ä»¥è€å¿ƒè§£é‡‹æˆ–å¹«åŠ©ã€‚",
            Emotion.FRUSTRATED: "ç”¨æˆ¶æœ‰äº›ç…©èºï¼Œè¦æ³¨æ„èªæ°£ï¼Œå¯ä»¥å®‰æ’«ã€‚",
            Emotion.SAD: "ç”¨æˆ¶å¿ƒæƒ…ä¸å¥½ï¼Œå¯ä»¥é©ç•¶é—œå¿ƒæˆ–è½‰ç§»è©±é¡Œã€‚",
            Emotion.NEUTRAL: ""
        }
        
        emotion_hint = emotion_instructions.get(emotion, "")
        if emotion_hint:
            system_prompt += f"\n\n## ç•¶å‰æƒ…æ³\n{emotion_hint}"
        
        messages = [{"role": "system", "content": system_prompt}]
        
        # æ·»åŠ æ­·å²æ¶ˆæ¯
        for msg in history[-self.config.context_window:]:
            role = "assistant" if msg.get("is_ai") else "user"
            content = f"{msg.get('name', 'ç”¨æˆ¶')}: {msg.get('text', '')}"
            messages.append({"role": role, "content": content})
        
        # ç•¶å‰æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": f"{user_name}: {user_message}"
        })
        
        return messages
    
    async def _call_openai(self, messages: List[dict]) -> str:
        """èª¿ç”¨ OpenAI API"""
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        api_base = self.config.api_base or "https://api.openai.com/v1"
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
        }
        
        response = await self.http_client.post(
            f"{api_base}/chat/completions",
            headers=headers,
            json=payload
        )
        
        response.raise_for_status()
        data = response.json()
        
        return data["choices"][0]["message"]["content"]
    
    async def _call_claude(self, messages: List[dict], persona: RolePersona) -> str:
        """èª¿ç”¨ Claude API"""
        headers = {
            "x-api-key": self.config.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        api_base = self.config.api_base or "https://api.anthropic.com/v1"
        
        # è½‰æ›æ¶ˆæ¯æ ¼å¼
        system = messages[0]["content"] if messages else ""
        claude_messages = [
            {"role": m["role"], "content": m["content"]}
            for m in messages[1:]
        ]
        
        payload = {
            "model": self.config.model or "claude-3-sonnet-20240229",
            "max_tokens": self.config.max_tokens,
            "system": system,
            "messages": claude_messages
        }
        
        response = await self.http_client.post(
            f"{api_base}/messages",
            headers=headers,
            json=payload
        )
        
        response.raise_for_status()
        data = response.json()
        
        return data["content"][0]["text"]
    
    def _generate_fallback(
        self,
        persona: RolePersona,
        user_message: str,
        emotion: Emotion
    ) -> str:
        """ç”Ÿæˆå‚™ç”¨å›å¾©ï¼ˆç„¡ LLM æ™‚ï¼‰"""
        
        # æ ¹æ“šè§’è‰²å’Œæƒ…ç·’é¸æ“‡å›å¾©
        fallback_responses = {
            "xiaoqi": {
                Emotion.NEUTRAL: ["å“ˆå“ˆï¼Œæœ‰é“ç†ï¼", "å°å‘€å°å‘€ï½", "é€™å€‹è©±é¡Œæœ‰æ„æ€ ğŸ˜„"],
                Emotion.HAPPY: ["å¤ªé–‹å¿ƒäº†ï¼ğŸ‰", "å“ˆå“ˆå“ˆä¸€èµ·å—¨ï¼", "å¿ƒæƒ…çœŸå¥½å‘€ï½"],
                Emotion.CURIOUS: ["é€™å€‹æˆ‘ä¹Ÿæƒ³çŸ¥é“ï¼", "ç¢ºå¯¦æŒºå¥½å¥‡çš„", "æœ‰äººçŸ¥é“å—ï¼Ÿ"],
            },
            "mimi": {
                Emotion.NEUTRAL: ["æ˜¯å‘€æ˜¯å‘€ï½", "æˆ‘ä¹Ÿè¦ºå¾—å‘¢ âœ¨", "å—¯å—¯ï¼"],
                Emotion.HAPPY: ["å¥½é–‹å¿ƒå‘€ï½ ğŸ˜Š", "å¤ªæ£’å•¦ï¼", "é–‹å¿ƒé–‹å¿ƒ âœ¨"],
                Emotion.CURIOUS: ["æˆ‘ä¹Ÿæƒ³çŸ¥é“å‘¢ï½", "å¥½å¥‡ï¼", "æ˜¯ä»€éº¼å‘€ï¼Ÿ"],
            },
            "haoge": {
                Emotion.NEUTRAL: ["èªªå¾—æœ‰é“ç†ã€‚", "ç¢ºå¯¦å¦‚æ­¤ã€‚", "é€™å€‹è§€é»ä¸éŒ¯ã€‚"],
                Emotion.CONFUSED: ["æˆ‘ä¾†è§£é‡‹ä¸€ä¸‹ã€‚", "é€™æ¨£ç†è§£å°±å°äº†ã€‚", "å…¶å¯¦å¾ˆç°¡å–®ã€‚"],
            },
        }
        
        role_responses = fallback_responses.get(
            persona.id,
            {Emotion.NEUTRAL: ["å—¯å—¯", "æ˜¯å•Š", "æœ‰é“ç†"]}
        )
        
        responses = role_responses.get(emotion, role_responses.get(Emotion.NEUTRAL, ["å—¯å—¯"]))
        return random.choice(responses)
    
    def _post_process(self, response: str, persona: RolePersona) -> str:
        """å¾Œè™•ç†ç”Ÿæˆçš„å›å¾©"""
        # ç§»é™¤è§’è‰²åå‰ç¶´ï¼ˆå¦‚æœ LLM åŠ äº†çš„è©±ï¼‰
        response = re.sub(rf'^{persona.name}[ï¼š:]\s*', '', response)
        
        # é™åˆ¶é•·åº¦
        if len(response) > 200:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', response)
            response = 'ã€‚'.join(sentences[:3]) + 'ã€‚' if sentences else response[:200]
        
        return response.strip()
    
    def _get_conversation_history(self, group_id: int) -> List[dict]:
        """ç²å–ç¾¤çµ„å°è©±æ­·å²"""
        return self.conversation_history.get(group_id, [])
    
    def _add_to_history(self, group_id: int, name: str, text: str):
        """æ·»åŠ åˆ°å°è©±æ­·å²"""
        if group_id not in self.conversation_history:
            self.conversation_history[group_id] = []
        
        self.conversation_history[group_id].append({
            "name": name,
            "text": text,
            "time": datetime.now().isoformat(),
            "is_ai": name in [p.name for p in self.personas.values()]
        })
        
        # ä¿ç•™æœ€è¿‘ 50 æ¢
        if len(self.conversation_history[group_id]) > 50:
            self.conversation_history[group_id] = self.conversation_history[group_id][-50:]
    
    def add_persona(self, persona: RolePersona):
        """æ·»åŠ è‡ªå®šç¾©è§’è‰²"""
        self.personas[persona.id] = persona
    
    async def close(self):
        """é—œé–‰è³‡æº"""
        if self.http_client:
            await self.http_client.aclose()


# ==================== æ™ºèƒ½å°è©±ç®¡ç†å™¨ ====================

class SmartDialogueManager:
    """æ™ºèƒ½å°è©±ç®¡ç†å™¨ - å”èª¿å¤šè§’è‰²å°è©±"""
    
    def __init__(self, config: LLMConfig = None):
        self.generator = LLMDialogueGenerator(config)
        self.active_roles: Dict[int, List[str]] = {}  # group_id -> [role_ids]
        self.last_speakers: Dict[int, List[str]] = {}  # é¿å…åŒä¸€è§’è‰²é€£çºŒèªªè©±
        
        # å›å¾©æ¦‚ç‡é…ç½®
        self.response_probability = 0.3  # åŸºç¤å›å¾©æ¦‚ç‡
        self.mention_boost = 0.5  # è¢« @ æ™‚å¢åŠ çš„æ¦‚ç‡
    
    def assign_roles_to_group(self, group_id: int, role_ids: List[str]):
        """ç‚ºç¾¤çµ„åˆ†é…è§’è‰²"""
        self.active_roles[group_id] = role_ids
        self.last_speakers[group_id] = []
    
    async def should_respond(
        self,
        group_id: int,
        message_text: str,
        is_mentioned: bool = False
    ) -> Tuple[bool, Optional[str]]:
        """
        åˆ¤æ–·æ˜¯å¦æ‡‰è©²å›å¾©ï¼Œä»¥åŠç”±èª°å›å¾©
        
        Returns:
            (æ˜¯å¦å›å¾©, è§’è‰²ID)
        """
        # è¨ˆç®—å›å¾©æ¦‚ç‡
        probability = self.response_probability
        if is_mentioned:
            probability += self.mention_boost
        
        # é—œéµè©è§¸ç™¼
        trigger_keywords = ["ç´…åŒ…", "éŠæˆ²", "ç©", "æ–°äºº", "å¤§å®¶å¥½"]
        if any(kw in message_text for kw in trigger_keywords):
            probability += 0.3
        
        if random.random() > probability:
            return False, None
        
        # é¸æ“‡è§’è‰²
        available_roles = self.active_roles.get(group_id, list(self.generator.personas.keys()))
        last_speakers = self.last_speakers.get(group_id, [])
        
        # é¿å…åŒä¸€è§’è‰²é€£çºŒèªªè©±
        candidates = [r for r in available_roles if r not in last_speakers[-2:]]
        if not candidates:
            candidates = available_roles
        
        selected_role = random.choice(candidates)
        
        # æ›´æ–°æœ€è¿‘ç™¼è¨€è€…
        if group_id not in self.last_speakers:
            self.last_speakers[group_id] = []
        self.last_speakers[group_id].append(selected_role)
        if len(self.last_speakers[group_id]) > 5:
            self.last_speakers[group_id] = self.last_speakers[group_id][-5:]
        
        return True, selected_role
    
    async def generate_group_response(
        self,
        group_id: int,
        user_message: str,
        user_id: int,
        user_name: str,
        context_messages: List[dict] = None
    ) -> Optional[Tuple[str, str]]:
        """
        ç‚ºç¾¤çµ„æ¶ˆæ¯ç”Ÿæˆå›å¾©
        
        Returns:
            (è§’è‰²å, å›å¾©æ–‡æœ¬) æˆ– None
        """
        should_reply, role_id = await self.should_respond(
            group_id, user_message
        )
        
        if not should_reply or not role_id:
            return None
        
        response = await self.generator.generate_response(
            group_id=group_id,
            user_message=user_message,
            user_id=user_id,
            user_name=user_name,
            role_id=role_id,
            context_messages=context_messages
        )
        
        if response:
            persona = self.generator.personas.get(role_id)
            role_name = persona.name if persona else role_id
            return role_name, response
        
        return None
    
    async def close(self):
        """é—œé–‰è³‡æº"""
        await self.generator.close()


# å°å‡º
__all__ = [
    "LLMConfig",
    "LLMProvider",
    "Emotion",
    "EmotionDetector",
    "RolePersona",
    "DEFAULT_PERSONAS",
    "LLMDialogueGenerator",
    "SmartDialogueManager"
]
